<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Benjamin Lira, Asaf Mazar, … and Angela Duckworth">

<title>Large Language Models make human coding in the social sciences obsolete</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="00 report_files/libs/clipboard/clipboard.min.js"></script>
<script src="00 report_files/libs/quarto-html/quarto.js"></script>
<script src="00 report_files/libs/quarto-html/popper.min.js"></script>
<script src="00 report_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="00 report_files/libs/quarto-html/anchor.min.js"></script>
<link href="00 report_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="00 report_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="00 report_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="00 report_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="00 report_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Large Language Models make human coding in the social sciences obsolete</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Benjamin Lira, Asaf Mazar, … and Angela Duckworth </p>
          </div>
  </div>
    
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="abstract-title">Abstract</div>
    Scientific progress in the social sciences has relied, among other tools, on trained raters reading text and categorizing it. We argue that advances in natural language processing, especially the advent of large language models have made this practice obsolete. We show how repeated automatic ratings compared to content expert and externally valid expert ratings of personal qualities in college admissions. Compared to human raters, machine ratings can be more reliable, and orders of magnitude cheaper and faster than human raters. We offer recommendations on how to optimally do this, and provide an R package and tutorial for LLM coding.
  </div>
</div>

</header>

<section id="main" class="level1">
<h1>Main</h1>
<p>Social science as an enterprise depends on the quality of its methods. One key methodological approach in the field involves human rating or coding of open-ended text, which has been instrumental in generating insights in landmark research. For instance… Despite its usefulness, human coding can be prohibitive in terms of training, reliability, and time required for analysis.</p>
<p>Recently, advances in natural language processing (NLP) have provided ways to automate these processes, offering a more efficient alternative to human coding. Traditionally, NLP techniques have been employed to fine-tune models for classification tasks, such as detecting perseverance in admissions essays or predicting user satisfaction from customer reviews. However, these methods require expertise, human labels for initial training, and substantial computational resources.</p>
<p>The advent of zero-shot learning in large language models (LLMs) like GPT-4 promises to address these challenges by eliminating the need for pretraining and human labeling. By leveraging the vast knowledge already embedded in these models, researchers can potentially deploy LLMs for various classification tasks without incurring the costs associated with traditional fine-tuning processes.</p>
<p>During our preparation of this work, there have been reports suggesting that LLMs outperform human annotators without fine-tuning (i.e., zero-shot learning). However, these studies present three limitations: (1) They compare LLMs to Mechanical Turk workers, rather than trained raters, setting a lower bar for performance. (2) The majority of these studies focus on sentiment analysis, which is a relatively simple task for which there are already a number of validated dictionaries (e.g., LIWC) or transformer models. More complex constructshave a far more sparse landscape in terms of available measures. (3) There is no evidence on how LLMs perform with respect to demographic variables, which is a major concern in social science research. Demographic bias and social biases are known to be embedded within LLMs, raising questions about whether LLMs based rating would introduce demographic bias.</p>
<p>However, if chatbots have the ability to match the validity of human ratings without bias, absent human labels and pretraining, this might significantly accelerate the pace of scientific investigation. In this study, we test whether LLMs are ready for this leap. We collected several social science datasets where human ratings across a range of social science constructs—such as political ideology, moral values, and cognitive style—were used to predict meaningful outcomes. We re-ran these studies replacing the original human ratings with LLM ratings. We then evaluated how well the LLM ratings matched the human ratings, whether they introduced demographic bias, and whether they predicted outcomes as well as the human ratings. By employing a diverse and representative sample of textual data encompassing several disciplines and constructs, that were rated by trained coders rather than MTurkers, and examining the demographic impact of replacing human raters, we hope to provide a more comprehensive assessment of the potential of LLMs for social science research.</p>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<section id="llm-coding-matches-human-agreement" class="level2">
<h2 class="anchored" data-anchor-id="llm-coding-matches-human-agreement">LLM coding matches human agreement</h2>
<p>ChatGPT ratings are already quite reliable when using default parameters (temperature = 1). See <strong>?@fig-alphas</strong> for Cronbach’s alpha coefficients.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-alphas-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p></p><figcaption class="figure-caption">Figure&nbsp;1: <strong>ChatGPT codes are reliable.</strong> When coding each essay twice, alpha coefficients were higher than those obtained by undergraduates and admissions officers.</figcaption><p></p>
<p><img src="00-report_files/figure-html/fig-alphas-1.png" class="img-fluid figure-img" width="480"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div id="fig-alphas-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p></p><figcaption class="figure-caption">Figure&nbsp;2: <strong>ChatGPT codes are reliable.</strong> When coding each essay twice, alpha coefficients were higher than those obtained by undergraduates and admissions officers.</figcaption><p></p>
<p><img src="00-report_files/figure-html/fig-alphas-2.png" class="img-fluid figure-img" width="480"></p>
</figure>
</div>
</div>
</div>
<p>LLM coding correlates with human coding ChatGPT codes correlate somewhat weakly to some of the human codes. However, all correlations are positive, in the expected direction, and greater than discriminant correlations.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p></p><figcaption class="figure-caption"><strong>ChatGPT codes correlate with human ratings.</strong> Although correlations are not super high, they are all positive and in the expected direction</figcaption><p></p>
<p><img src="plots/correlations.png" class="img-fluid figure-img" width="750"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p></p><figcaption class="figure-caption"><strong>ChatGPT codes correlate with human ratings.</strong> Although correlations are not super high, they are all positive and in the expected direction</figcaption><p></p>
<p><img src="plots/correlations_rohrer.png" class="img-fluid figure-img" width="750"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="llm-coding-does-not-introduce-demographic-bias" class="level2">
<h2 class="anchored" data-anchor-id="llm-coding-does-not-introduce-demographic-bias">LLM coding does not introduce demographic bias</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p></p><figcaption class="figure-caption"><strong>ChatGPT codes do not introduce systematic bias in ratings.</strong> While there are</figcaption><p></p>
<p><img src="plots/correlations_demo.png" class="img-fluid figure-img" width="900"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="llm-coding-predicts-outcomes-as-well-as-do-human-codes" class="level2">
<h2 class="anchored" data-anchor-id="llm-coding-predicts-outcomes-as-well-as-do-human-codes">LLM coding predicts outcomes as well as do human codes</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p></p><figcaption class="figure-caption"><strong>Relationships between human codes and chatbot codes and outcome</strong></figcaption><p></p>
<p><img src="plots/correlations_out.png" class="img-fluid figure-img" width="900"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p></p><figcaption class="figure-caption"><strong>Relationships between human codes and chatbot codes and outcome</strong></figcaption><p></p>
<p><img src="plots/correlations_out_rohrer.png" class="img-fluid figure-img" width="900"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p>We show that fully or partially automated methods can challenge the importance of human coding in the social sciences. They can produce scores more reliable than human raters, at lower cost, and almost instantaneously, without the need for extra finetuning and human labelling. Coding data with LLMs does not introduce demographic biases, and codes retain their predictive accuracy. Finally, across constructs, it seems that more abstract coding is a harder problem for LLMs.</p>
<p>Contributions The time and money savings are massive. In less than an hour, ChatGPT read 3 thousand essays, and did so for around 10 dollars. ChatGPT looks not so great if you assume that human raters are noiseless and unbiased. This is a bold assumption, that when relaxed makes chatGPT look so much better.</p>
<p>Limitations It is hard to know whether results generalize to other kinds of constructs, elicitation procedures, and rating schemes.</p>
<p>Future directions Adaptive measurement</p>
<p>The fact that LLMs are not always open-source, means that if they were to stop being supported, research might fail to be reproducible in the long run. The inherent noise in LLMs also makes it difficult to use them for reproducibility (i.e., another researcher might get different ratings when trying to replicate findings).</p>
<p>Implications</p>
</section>
<section id="methods" class="level1">
<h1>Methods</h1>
</section>
<section id="scraps" class="level1">
<h1>Scraps</h1>
<p>Text completion engines have noise built into them, so their output is not deterministic. This makes it so that the ratings do not have perfect reliability. However, if they are unbiased, noise can be averaged out through aggregation.</p>
<p>Another relevant point is where coded constructs lie in the concrete-abstract dimension. It is likely that LLMs will perform better for more concrete constructs (e.g., altruism) vs more abstract ones (e.g., intrinsic motivation).</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>